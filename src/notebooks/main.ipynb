{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project_Break_II_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Personalized Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from utils.User_Functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "try_GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6743404 entries, 0 to 6743403\n",
      "Data columns (total 24 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   FlightDate          object\n",
      " 1   Day_Of_Week         int64 \n",
      " 2   Airline             object\n",
      " 3   Tail_Number         object\n",
      " 4   Dep_Airport         object\n",
      " 5   Dep_CityName        object\n",
      " 6   DepTime_label       object\n",
      " 7   Dep_Delay           int64 \n",
      " 8   Dep_Delay_Tag       int64 \n",
      " 9   Dep_Delay_Type      object\n",
      " 10  Arr_Airport         object\n",
      " 11  Arr_CityName        object\n",
      " 12  Arr_Delay           int64 \n",
      " 13  Arr_Delay_Type      object\n",
      " 14  Flight_Duration     int64 \n",
      " 15  Distance_type       object\n",
      " 16  Delay_Carrier       int64 \n",
      " 17  Delay_Weather       int64 \n",
      " 18  Delay_NAS           int64 \n",
      " 19  Delay_Security      int64 \n",
      " 20  Delay_LastAircraft  int64 \n",
      " 21  Manufacturer        object\n",
      " 22  Model               object\n",
      " 23  Aicraft_age         int64 \n",
      "dtypes: int64(11), object(13)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "df_flights = pd.read_csv(\"./src/data/US_flights_2023.csv\")\n",
    "df_flights.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132860 entries, 0 to 132859\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   time        132860 non-null  object \n",
      " 1   tavg        132860 non-null  float64\n",
      " 2   tmin        132860 non-null  float64\n",
      " 3   tmax        132860 non-null  float64\n",
      " 4   prcp        132860 non-null  float64\n",
      " 5   snow        132860 non-null  float64\n",
      " 6   wdir        132860 non-null  float64\n",
      " 7   wspd        132860 non-null  float64\n",
      " 8   pres        132860 non-null  float64\n",
      " 9   airport_id  132860 non-null  object \n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_weather = pd.read_csv(\"./src/data/weather_meteo_by_airport.csv\")\n",
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assumindo que você já tem os dados carregados nas variáveis:\n",
    "# flights_df (dados de voos)\n",
    "# weather_df (dados de clima)\n",
    "# location_df (dados de localização)\n",
    "\n",
    "# Convertendo as colunas de data para datetime, caso necessário\n",
    "df_flights['FlightDate'] = pd.to_datetime(df_flights['FlightDate'], errors='coerce')\n",
    "df_weather['time'] = pd.to_datetime(df_weather['time'], errors='coerce')\n",
    "\n",
    "# Realizando o primeiro merge (Voos + Meteorologia)\n",
    "merged_df = pd.merge(df_flights, df_weather,\n",
    "                      how='left',\n",
    "                      left_on=['Dep_Airport', 'FlightDate'],\n",
    "                      right_on=['airport_id', 'time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Realizando o segundo merge (Voos + Meteorologia + Localização)\n",
    "# final_df = pd.merge(merged_df, location_df,\n",
    "#                      how='left',\n",
    "#                      left_on=['Dep_Airport'],\n",
    "#                      right_on=['IATA_CODE'])\n",
    "\n",
    "# # Verificando o resultado do segundo merge\n",
    "# print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FlightDate', 'Day_Of_Week', 'Airline', 'Tail_Number', 'Dep_Airport',\n",
       "       'Dep_CityName', 'DepTime_label', 'Dep_Delay', 'Dep_Delay_Tag',\n",
       "       'Dep_Delay_Type', 'Arr_Airport', 'Arr_CityName', 'Arr_Delay',\n",
       "       'Arr_Delay_Type', 'Flight_Duration', 'Distance_type', 'Delay_Carrier',\n",
       "       'Delay_Weather', 'Delay_NAS', 'Delay_Security', 'Delay_LastAircraft',\n",
       "       'Manufacturer', 'Model', 'Aicraft_age', 'time', 'tavg', 'tmin', 'tmax',\n",
       "       'prcp', 'snow', 'wdir', 'wspd', 'pres', 'airport_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6743404 entries, 0 to 6743403\n",
      "Data columns (total 34 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   FlightDate          datetime64[ns]\n",
      " 1   Day_Of_Week         int64         \n",
      " 2   Airline             object        \n",
      " 3   Tail_Number         object        \n",
      " 4   Dep_Airport         object        \n",
      " 5   Dep_CityName        object        \n",
      " 6   DepTime_label       object        \n",
      " 7   Dep_Delay           int64         \n",
      " 8   Dep_Delay_Tag       int64         \n",
      " 9   Dep_Delay_Type      object        \n",
      " 10  Arr_Airport         object        \n",
      " 11  Arr_CityName        object        \n",
      " 12  Arr_Delay           int64         \n",
      " 13  Arr_Delay_Type      object        \n",
      " 14  Flight_Duration     int64         \n",
      " 15  Distance_type       object        \n",
      " 16  Delay_Carrier       int64         \n",
      " 17  Delay_Weather       int64         \n",
      " 18  Delay_NAS           int64         \n",
      " 19  Delay_Security      int64         \n",
      " 20  Delay_LastAircraft  int64         \n",
      " 21  Manufacturer        object        \n",
      " 22  Model               object        \n",
      " 23  Aicraft_age         int64         \n",
      " 24  time                datetime64[ns]\n",
      " 25  tavg                float64       \n",
      " 26  tmin                float64       \n",
      " 27  tmax                float64       \n",
      " 28  prcp                float64       \n",
      " 29  snow                float64       \n",
      " 30  wdir                float64       \n",
      " 31  wspd                float64       \n",
      " 32  pres                float64       \n",
      " 33  airport_id          object        \n",
      "dtypes: datetime64[ns](2), float64(8), int64(11), object(13)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Tail_Number</th>\n",
       "      <th>Dep_Airport</th>\n",
       "      <th>Dep_CityName</th>\n",
       "      <th>DepTime_label</th>\n",
       "      <th>Dep_Delay</th>\n",
       "      <th>Dep_Delay_Tag</th>\n",
       "      <th>Dep_Delay_Type</th>\n",
       "      <th>Arr_Airport</th>\n",
       "      <th>...</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>airport_id</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5771091</th>\n",
       "      <td>2</td>\n",
       "      <td>Delta Air Lines Inc</td>\n",
       "      <td>N320US</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Evening</td>\n",
       "      <td>-21</td>\n",
       "      <td>0</td>\n",
       "      <td>Low &lt;5min</td>\n",
       "      <td>LAX</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>AUS</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360751</th>\n",
       "      <td>5</td>\n",
       "      <td>Southwest Airlines Co.</td>\n",
       "      <td>N7735A</td>\n",
       "      <td>TUS</td>\n",
       "      <td>Tucson, AZ</td>\n",
       "      <td>Morning</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Medium &gt;15min</td>\n",
       "      <td>DEN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.2</td>\n",
       "      <td>37.2</td>\n",
       "      <td>21.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>TUS</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Day_Of_Week                 Airline Tail_Number Dep_Airport  \\\n",
       "5771091            2     Delta Air Lines Inc      N320US         AUS   \n",
       "4360751            5  Southwest Airlines Co.      N7735A         TUS   \n",
       "\n",
       "        Dep_CityName DepTime_label  Dep_Delay  Dep_Delay_Tag Dep_Delay_Type  \\\n",
       "5771091   Austin, TX       Evening        -21              0      Low <5min   \n",
       "4360751   Tucson, AZ       Morning         22              1  Medium >15min   \n",
       "\n",
       "        Arr_Airport  ...  tmin  tmax  prcp  snow   wdir  wspd    pres  \\\n",
       "5771091         LAX  ...  17.8  29.4   0.0   0.0  189.0  18.0  1013.0   \n",
       "4360751         DEN  ...  22.2  37.2  21.3   0.0  141.0  15.8  1010.7   \n",
       "\n",
       "         airport_id  Month  Day  \n",
       "5771091         AUS     11    7  \n",
       "4360751         TUS      8   18  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Month'] = merged_df['FlightDate'].dt.month\n",
    "merged_df['Day'] = merged_df['FlightDate'].dt.day\n",
    "merged_df.drop(columns=[\"FlightDate\", \"time\"], inplace=True)\n",
    "merged_df.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features numéricas: ['Day_Of_Week', 'Dep_Delay', 'Dep_Delay_Tag', 'Arr_Delay', 'Flight_Duration', 'Delay_Carrier', 'Delay_Weather', 'Delay_NAS', 'Delay_Security', 'Delay_LastAircraft', 'Aicraft_age', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'pres', 'Month', 'Day']\n",
      "Features categóricas: ['Airline', 'Tail_Number', 'Dep_Airport', 'Dep_CityName', 'DepTime_label', 'Dep_Delay_Type', 'Arr_Airport', 'Arr_CityName', 'Arr_Delay_Type', 'Distance_type', 'Manufacturer', 'Model', 'airport_id']\n",
      "Features ID (categóricas numéricas): []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que você já tenha carregado o DataFrame (df)\n",
    "# df = pd.read_csv('seu_arquivo.csv')\n",
    "\n",
    "# Inicializando as listas para cada tipo de feature\n",
    "features_num = []  # Para colunas numéricas\n",
    "features_cat = []  # Para colunas categóricas\n",
    "features_id = []   # Para colunas categóricas que podem ser IDs (números representando IDs)\n",
    "\n",
    "# Loop para classificar as colunas\n",
    "for column in merged_df.columns:\n",
    "    dtype = merged_df[column].dtype\n",
    "    \n",
    "    # Ignorar colunas datetime\n",
    "    if pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        continue\n",
    "    \n",
    "    # Verificar se a coluna é numérica\n",
    "    if pd.api.types.is_numeric_dtype(dtype):\n",
    "        features_num.append(column)\n",
    "    # Verificar se a coluna é categórica (objeto ou categorias)\n",
    "    elif pd.api.types.is_object_dtype(dtype):\n",
    "        # Verificar se a coluna pode ser considerada um ID (normalmente números representados como string)\n",
    "        if merged_df[column].str.isnumeric().all():\n",
    "            features_id.append(column)\n",
    "        else:\n",
    "            features_cat.append(column)\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Features numéricas:\", features_num)\n",
    "print(\"Features categóricas:\", features_cat)\n",
    "print(\"Features ID (categóricas numéricas):\", features_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Tail_Number</th>\n",
       "      <th>Dep_Airport</th>\n",
       "      <th>Dep_CityName</th>\n",
       "      <th>DepTime_label</th>\n",
       "      <th>Dep_Delay</th>\n",
       "      <th>Dep_Delay_Tag</th>\n",
       "      <th>Dep_Delay_Type</th>\n",
       "      <th>Arr_Airport</th>\n",
       "      <th>Arr_CityName</th>\n",
       "      <th>Arr_Delay</th>\n",
       "      <th>Arr_Delay_Type</th>\n",
       "      <th>Flight_Duration</th>\n",
       "      <th>Distance_type</th>\n",
       "      <th>Delay_Carrier</th>\n",
       "      <th>Delay_Weather</th>\n",
       "      <th>Delay_NAS</th>\n",
       "      <th>Delay_Security</th>\n",
       "      <th>Delay_LastAircraft</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Model</th>\n",
       "      <th>Aicraft_age</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>airport_id</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2131154</th>\n",
       "      <td>4</td>\n",
       "      <td>PSA Airlines</td>\n",
       "      <td>N584NN</td>\n",
       "      <td>CLT</td>\n",
       "      <td>Charlotte, NC</td>\n",
       "      <td>Morning</td>\n",
       "      <td>-7</td>\n",
       "      <td>0</td>\n",
       "      <td>Low &lt;5min</td>\n",
       "      <td>AVL</td>\n",
       "      <td>Asheville, NC</td>\n",
       "      <td>-4</td>\n",
       "      <td>Low &lt;5min</td>\n",
       "      <td>61</td>\n",
       "      <td>Short Haul &gt;1500Mi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CANADAIR REGIONAL JET</td>\n",
       "      <td>CRJ</td>\n",
       "      <td>9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1022.6</td>\n",
       "      <td>CLT</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622781</th>\n",
       "      <td>7</td>\n",
       "      <td>Alaska Airlines Inc.</td>\n",
       "      <td>N491AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Morning</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>Low &lt;5min</td>\n",
       "      <td>LAS</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>-9</td>\n",
       "      <td>Low &lt;5min</td>\n",
       "      <td>144</td>\n",
       "      <td>Short Haul &gt;1500Mi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BOEING</td>\n",
       "      <td>737 NG</td>\n",
       "      <td>9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1024.2</td>\n",
       "      <td>SEA</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Day_Of_Week               Airline Tail_Number Dep_Airport  \\\n",
       "2131154            4          PSA Airlines      N584NN         CLT   \n",
       "622781             7  Alaska Airlines Inc.      N491AS         SEA   \n",
       "\n",
       "          Dep_CityName DepTime_label  Dep_Delay  Dep_Delay_Tag Dep_Delay_Type  \\\n",
       "2131154  Charlotte, NC       Morning         -7              0      Low <5min   \n",
       "622781     Seattle, WA       Morning         -3              0      Low <5min   \n",
       "\n",
       "        Arr_Airport   Arr_CityName  Arr_Delay Arr_Delay_Type  Flight_Duration  \\\n",
       "2131154         AVL  Asheville, NC         -4      Low <5min               61   \n",
       "622781          LAS  Las Vegas, NV         -9      Low <5min              144   \n",
       "\n",
       "              Distance_type  Delay_Carrier  Delay_Weather  Delay_NAS  \\\n",
       "2131154  Short Haul >1500Mi              0              0          0   \n",
       "622781   Short Haul >1500Mi              0              0          0   \n",
       "\n",
       "         Delay_Security  Delay_LastAircraft           Manufacturer   Model  \\\n",
       "2131154               0                   0  CANADAIR REGIONAL JET     CRJ   \n",
       "622781                0                   0                 BOEING  737 NG   \n",
       "\n",
       "         Aicraft_age  tavg  tmin  tmax  prcp  snow   wdir  wspd    pres  \\\n",
       "2131154            9  22.1  14.4  29.4   0.0   0.0  212.0  11.2  1022.6   \n",
       "622781             9   6.2   4.4   8.9   1.3   0.0  185.0  15.8  1024.2   \n",
       "\n",
       "        airport_id  Month  Day  \n",
       "2131154        CLT      4   20  \n",
       "622781         SEA      2   19  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "merged_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis escolhidas a priori para modelo de Classificação sem inclusão de dados de delay e cancelamento\n",
    "# nem nada de vôos já iniciados, apenas programados\n",
    "\n",
    "# IMPORTANTE --> ESTOU INSERINDO Delay_LastAircraft, QUE É O NÚMERO DE MINUTOS DE ATRASO DA AERONAVE NO VÔO ANTERIOR.\n",
    "# CONSIDERAR NÃO CONSIDERÁ-LA\n",
    "\n",
    "# features_num = [\"Month\", \"Day\", \"Day_Of_Week\", \"Aicraft_age\", \"Delay_LastAircraft\", \"tmin\", \"tmax\", \"prcp\", \n",
    "#                 \"snow\", \"wdir\", \"wspd\", \"pres\"]\n",
    "\n",
    "\n",
    "features_num = [\"Month\", \"Day\", \"Day_Of_Week\", \"Aicraft_age\", \"tmin\", \"tmax\", \"prcp\", \n",
    "                \"snow\", \"wdir\", \"wspd\", \"pres\"]\n",
    "\n",
    "features_cat = [\"Airline\", \"Tail_Number\", \"Dep_Airport\", \"DepTime_label\", \"Arr_Airport\", \"Distance_type\", \n",
    "                \"Manufacturer\", \"Model\", ]\n",
    "\n",
    "#features_id = []\n",
    "\n",
    "categorical_features = features_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "\n",
    "for col in features_id:\n",
    "    df[col] = df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = \"Dep_Delay_Tag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month            0\n",
       "Day              0\n",
       "Day_Of_Week      0\n",
       "Aicraft_age      0\n",
       "tmin             0\n",
       "tmax             0\n",
       "prcp             0\n",
       "snow             0\n",
       "wdir             0\n",
       "wspd             0\n",
       "pres             0\n",
       "Airline          0\n",
       "Tail_Number      0\n",
       "Dep_Airport      0\n",
       "DepTime_label    0\n",
       "Arr_Airport      0\n",
       "Distance_type    0\n",
       "Manufacturer     0\n",
       "Model            0\n",
       "Dep_Delay_Tag    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[features_num + features_cat + [target_class]].copy()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dep_Delay_Tag\n",
       "0    4187645\n",
       "1    2555759\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target_class].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding with K-Fold\n",
    "\n",
    "The **`target_encoding_kfold`** function applies **Target Encoding** with **K-Fold Cross Validation** to prevent **data leakage** during training, ensuring the model only learns from training data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Steps:\n",
    "\n",
    "1. **K-Fold Validation**:\n",
    "   - Splits the training data into **n_splits** (default 5) using **StratifiedKFold** to ensure balanced target distribution.\n",
    "   - Prevents **data leakage** by ensuring encoding is only based on training data.\n",
    "\n",
    "2. **Resetting Indices**:\n",
    "   - Resets indices in both train and validation sets to avoid **index misalignment** when applying target encoding.\n",
    "\n",
    "3. **Smoothing**:\n",
    "   - Calculates the category mean with a smoothing technique that combines the category mean with the global mean to handle sparse categories.\n",
    "\n",
    "4. **Mapping and Handling Unseen Categories**:\n",
    "   - Categories in validation and test sets are **mapped** to the calculated target mean; missing categories are assigned the **global mean** to avoid errors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why K-Fold?\n",
    "\n",
    "- **K-Fold** ensures that encoding is done only with training data, preventing future data from contaminating the model and causing **data leakage**.\n",
    "\n",
    "---\n",
    "\n",
    "This approach guarantees **robust target encoding**, preserves model integrity, and prevents **overfitting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5394723, 20)\n",
      "(1348681, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Tail_Number</th>\n",
       "      <th>Dep_Airport</th>\n",
       "      <th>DepTime_label</th>\n",
       "      <th>Arr_Airport</th>\n",
       "      <th>Distance_type</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Airline  Tail_Number  Dep_Airport  DepTime_label  Arr_Airport  \\\n",
       "unique          0            0            0              0            0   \n",
       "absolute        0            0            0              0            0   \n",
       "\n",
       "          Distance_type  Manufacturer  Model  \n",
       "unique                0             0      0  \n",
       "absolute              0             0      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check non correspondenting values in test set/train set\n",
    "non_correspondence = {col: {'unique': [], 'absolute': []} for col in categorical_features}\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for col in categorical_features:\n",
    "    # Identify unique values in test_set that are not in train_set\n",
    "    test_values = set(test_set[col])\n",
    "    train_values = set(train_set[col])\n",
    "    \n",
    "    # Values in test_set but not in train_set\n",
    "    non_matching_values = test_values - train_values\n",
    "    \n",
    "    # Count the number of unique values and the absolute count of these values\n",
    "    non_correspondence[col]['unique'] = len(non_matching_values)\n",
    "    non_correspondence[col]['absolute'] = sum(test_set[col].isin(non_matching_values))\n",
    "    \n",
    "# Convert the dictionary to a DataFrame\n",
    "non_correspondence_df = pd.DataFrame(non_correspondence)\n",
    "non_correspondence_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set size: 5394723\n",
      "Training set size after subsampling: 269736\n"
     ]
    }
   ],
   "source": [
    "# Subamostragem temporária para reduzir número de dados no treinamento e agilizar\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Random subsampling: reduces the training set size to 50%\n",
    "train_set_subsampled = resample(train_set, replace=False, n_samples=int(len(train_set) * 0.05), random_state=42)\n",
    "\n",
    "print(\"Original training set size:\", len(train_set))\n",
    "print(\"Training set size after subsampling:\", len(train_set_subsampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded, encoded_columns = target_encoding_kfold(train_set_subsampled, test_set, categorical_features, target_class, n_splits=5, smoothing=0.3, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually expected value: 0.37594970423945245\n",
      "Value assigned in train_encoded: [0.37537071 0.37901833 0.37755796 0.37861156 0.37667327]\n"
     ]
    }
   ],
   "source": [
    "col = \"Airline\"  # Choose a categorical column\n",
    "chosen_category = \"American Airlines Inc.\"  # Choose a specific category\n",
    "\n",
    "# Filter the training set only for this category\n",
    "df_category = train_set[train_set[col] == chosen_category]\n",
    "\n",
    "# Manual calculation\n",
    "count = len(df_category)\n",
    "mean = df_category[target_class].mean()\n",
    "smoothing_factor = 0.3\n",
    "global_mean = train_set[target_class].mean()\n",
    "\n",
    "category_te_manual = (mean * count + global_mean * smoothing_factor) / (count + smoothing_factor)\n",
    "\n",
    "print(\"Manually expected value:\", category_te_manual)\n",
    "print(\"Value assigned in train_encoded:\", train_encoded[train_encoded[col] == chosen_category][col + \"_te\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_encoded: (269736, 19)\n",
      "y_train: (269736,)\n",
      "X_test_encoded: (1348681, 19)\n",
      "y_test: (1348681,)\n"
     ]
    }
   ],
   "source": [
    "X_train_encoded = train_encoded[features_num + encoded_columns].copy()\n",
    "X_test_encoded = test_encoded[features_num + encoded_columns].copy()\n",
    "y_train = train_encoded[target_class]\n",
    "y_test = test_encoded[target_class]\n",
    "\n",
    "\n",
    "print(\"X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "\n",
    "print(\"X_test_encoded:\", X_test_encoded.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Month', 'Day', 'Day_Of_Week', 'Aicraft_age', 'tmin', 'tmax', 'prcp',\n",
       "       'snow', 'wdir', 'wspd', 'pres', 'Airline', 'Tail_Number', 'Dep_Airport',\n",
       "       'DepTime_label', 'Arr_Airport', 'Distance_type', 'Manufacturer',\n",
       "       'Model', 'Dep_Delay_Tag', 'Airline_te', 'Tail_Number_te',\n",
       "       'Dep_Airport_te', 'DepTime_label_te', 'Arr_Airport_te',\n",
       "       'Distance_type_te', 'Manufacturer_te', 'Model_te'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Airline_te  Tail_Number_te  Dep_Airport_te  \\\n",
      "Airline_te          1.000000        0.737675        0.397889   \n",
      "Tail_Number_te      0.737675        1.000000        0.327973   \n",
      "Dep_Airport_te      0.397889        0.327973        1.000000   \n",
      "DepTime_label_te    0.003556        0.006225        0.057670   \n",
      "Arr_Airport_te      0.475182        0.376973        0.141233   \n",
      "Distance_type_te    0.075521        0.089042        0.075592   \n",
      "Manufacturer_te     0.742372        0.554462        0.323680   \n",
      "Model_te            0.767109        0.612333        0.339442   \n",
      "\n",
      "                  DepTime_label_te  Arr_Airport_te  Distance_type_te  \\\n",
      "Airline_te                0.003556        0.475182          0.075521   \n",
      "Tail_Number_te            0.006225        0.376973          0.089042   \n",
      "Dep_Airport_te            0.057670        0.141233          0.075592   \n",
      "DepTime_label_te          1.000000        0.004141         -0.005777   \n",
      "Arr_Airport_te            0.004141        1.000000          0.064972   \n",
      "Distance_type_te         -0.005777        0.064972          1.000000   \n",
      "Manufacturer_te          -0.002389        0.338692          0.209695   \n",
      "Model_te                 -0.003181        0.359396          0.243205   \n",
      "\n",
      "                  Manufacturer_te  Model_te  \n",
      "Airline_te               0.742372  0.767109  \n",
      "Tail_Number_te           0.554462  0.612333  \n",
      "Dep_Airport_te           0.323680  0.339442  \n",
      "DepTime_label_te        -0.002389 -0.003181  \n",
      "Arr_Airport_te           0.338692  0.359396  \n",
      "Distance_type_te         0.209695  0.243205  \n",
      "Manufacturer_te          1.000000  0.904343  \n",
      "Model_te                 0.904343  1.000000  \n"
     ]
    }
   ],
   "source": [
    "columns_to_check = ['Airline_te',\n",
    "       'Tail_Number_te', 'Dep_Airport_te', 'DepTime_label_te',\n",
    "       'Arr_Airport_te', 'Distance_type_te', 'Manufacturer_te', 'Model_te'] \n",
    "\n",
    "# Correlation Matrix\n",
    "corr_matrix = X_train_encoded[columns_to_check].corr()\n",
    "# Exibe a matriz\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269736, 17)\n",
      "(1348681, 17)\n"
     ]
    }
   ],
   "source": [
    "# Dropping high correlated columns\n",
    "X_train_encoded.drop(columns=[\"Tail_Number_te\", \"Manufacturer_te\"], inplace=True)\n",
    "X_test_encoded.drop(columns=[\"Tail_Number_te\", \"Manufacturer_te\"], inplace=True)\n",
    "\n",
    "print(X_train_encoded.shape)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandartScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled = scale_features(X_train_encoded, X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Day_Of_Week</th>\n",
       "      <th>Aicraft_age</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>Airline_te</th>\n",
       "      <th>Dep_Airport_te</th>\n",
       "      <th>DepTime_label_te</th>\n",
       "      <th>Arr_Airport_te</th>\n",
       "      <th>Distance_type_te</th>\n",
       "      <th>Model_te</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60190</th>\n",
       "      <td>-1.350141</td>\n",
       "      <td>0.940971</td>\n",
       "      <td>0.508561</td>\n",
       "      <td>0.317874</td>\n",
       "      <td>-0.983284</td>\n",
       "      <td>-0.511166</td>\n",
       "      <td>-0.295448</td>\n",
       "      <td>-0.10401</td>\n",
       "      <td>1.051531</td>\n",
       "      <td>1.294266</td>\n",
       "      <td>1.372936</td>\n",
       "      <td>1.355149</td>\n",
       "      <td>2.517992</td>\n",
       "      <td>-1.098421</td>\n",
       "      <td>1.303454</td>\n",
       "      <td>-0.394715</td>\n",
       "      <td>0.688961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Month       Day  Day_Of_Week  Aicraft_age      tmin      tmax  \\\n",
       "60190 -1.350141  0.940971     0.508561     0.317874 -0.983284 -0.511166   \n",
       "\n",
       "           prcp     snow      wdir      wspd      pres  Airline_te  \\\n",
       "60190 -0.295448 -0.10401  1.051531  1.294266  1.372936    1.355149   \n",
       "\n",
       "       Dep_Airport_te  DepTime_label_te  Arr_Airport_te  Distance_type_te  \\\n",
       "60190        2.517992         -1.098421        1.303454         -0.394715   \n",
       "\n",
       "       Model_te  \n",
       "60190  0.688961  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling with NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set shape: (269736, 17)\n",
      "\n",
      "Resampled version 1 shape: (204872, 17)\n",
      "Resampled version 3 shape: (204872, 17)\n"
     ]
    }
   ],
   "source": [
    "# NearMiss\n",
    "\n",
    "# Aplicar NearMiss para diferentes versões\n",
    "nm_1 = NearMiss(version=1)  # Version 1 selects the samples closest to the minority class\n",
    "nm_3 = NearMiss(version=3)  # Version 3 selects both close and distant samples from the minority class\n",
    "\n",
    "# Resampling\n",
    "X_train_resampled_1, y_train_resampled_1 = nm_1.fit_resample(X_train_scaled, y_train)\n",
    "X_train_resampled_3, y_train_resampled_3 = nm_3.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Creating DataFrames\n",
    "df_X_train_resampled_1 = pd.DataFrame(X_train_resampled_1, columns=X_train_scaled.columns)\n",
    "df_y_train_resampled_1 = pd.DataFrame(y_train_resampled_1, columns=[target_class])\n",
    "\n",
    "df_X_train_resampled_3 = pd.DataFrame(X_train_resampled_3, columns=X_train_scaled.columns)\n",
    "df_y_train_resampled_3 = pd.DataFrame(y_train_resampled_3, columns=[target_class])\n",
    "\n",
    "# Print the shape of the resampled data\n",
    "print(f\"Original training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"\\nResampled version 1 shape: {df_X_train_resampled_1.shape}\")\n",
    "print(f\"Resampled version 3 shape: {df_X_train_resampled_3.shape}\")\n",
    "\n",
    "# Saving DataFrames\n",
    "df_X_train_resampled_1.to_csv('X_train_resampled_version_1.csv', index=False)\n",
    "df_y_train_resampled_1.to_csv('y_train_resampled_version_1.csv', index=False)\n",
    "\n",
    "df_X_train_resampled_3.to_csv('X_train_resampled_version_3.csv', index=False)\n",
    "df_y_train_resampled_3.to_csv('y_train_resampled_version_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved files\n",
    "X_train_resampled_1 = pd.read_csv('X_train_resampled_version_1.csv')\n",
    "y_train_resampled_1 = pd.read_csv('y_train_resampled_version_1.csv')\n",
    "\n",
    "X_train_resampled_3 = pd.read_csv('X_train_resampled_version_3.csv')\n",
    "y_train_resampled_3 = pd.read_csv('y_train_resampled_version_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.50      0.55    838275\n",
      "           1       0.38      0.50      0.43    510406\n",
      "\n",
      "    accuracy                           0.50   1348681\n",
      "   macro avg       0.50      0.50      0.49   1348681\n",
      "weighted avg       0.53      0.50      0.51   1348681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Conta a proporção real das classes no conjunto de treinamento\n",
    "unique, counts = np.unique(y_train_resampled_1, return_counts=True)\n",
    "class_proportions = counts / counts.sum()\n",
    "\n",
    "# Gera previsões aleatórias seguindo a proporção das classes\n",
    "y_pred_random = np.random.choice(unique, size=len(y_test), p=class_proportions)\n",
    "\n",
    "# Avalia o desempenho do classificador aleatório\n",
    "print(\"Random Classifier - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\GPU-TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.64    204945\n",
      "           1       0.63      0.60      0.62    204945\n",
      "\n",
      "    accuracy                           0.63    409890\n",
      "   macro avg       0.63      0.63      0.63    409890\n",
      "weighted avg       0.63      0.63      0.63    409890\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67    838275\n",
      "           1       0.50      0.60      0.55    510406\n",
      "\n",
      "    accuracy                           0.62   1348681\n",
      "   macro avg       0.61      0.62      0.61   1348681\n",
      "weighted avg       0.64      0.62      0.63   1348681\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating QDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\GPU-TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.95      0.69    204945\n",
      "           1       0.79      0.19      0.31    204945\n",
      "\n",
      "    accuracy                           0.57    409890\n",
      "   macro avg       0.66      0.57      0.50    409890\n",
      "weighted avg       0.66      0.57      0.50    409890\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.87      0.74    838275\n",
      "           1       0.48      0.20      0.28    510406\n",
      "\n",
      "    accuracy                           0.62   1348681\n",
      "   macro avg       0.56      0.53      0.51   1348681\n",
      "weighted avg       0.58      0.62      0.56   1348681\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# NearMiss Version 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define models with default hyperparameters\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    #\"SVM\": SVC(random_state=42),\n",
    "    #\"KNN\": KNeighborsClassifier(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, classification_report(y_train, y_pred_train), classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Loop through each model and evaluate it\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    y_pred_train, y_pred_test, train_report, test_report = evaluate_model(model, X_train_resampled_1, y_train_resampled_1, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nTrain Classification Report:\")\n",
    "    print(train_report)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(\"Test Classification Report:\")\n",
    "    print(test_report)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LogisticRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\GPU-TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59    204945\n",
      "           1       0.60      0.61      0.60    204945\n",
      "\n",
      "    accuracy                           0.60    409890\n",
      "   macro avg       0.60      0.60      0.60    409890\n",
      "weighted avg       0.60      0.60      0.60    409890\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69    838275\n",
      "           1       0.52      0.61      0.56    510406\n",
      "\n",
      "    accuracy                           0.64   1348681\n",
      "   macro avg       0.63      0.63      0.62   1348681\n",
      "weighted avg       0.65      0.64      0.64   1348681\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating QDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\anaconda3\\envs\\GPU-TensorFlow\\lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.61    204945\n",
      "           1       0.60      0.59      0.60    204945\n",
      "\n",
      "    accuracy                           0.60    409890\n",
      "   macro avg       0.60      0.60      0.60    409890\n",
      "weighted avg       0.60      0.60      0.60    409890\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70    838275\n",
      "           1       0.52      0.59      0.55    510406\n",
      "\n",
      "    accuracy                           0.64   1348681\n",
      "   macro avg       0.63      0.63      0.63   1348681\n",
      "weighted avg       0.65      0.64      0.65   1348681\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# NearMiss Version 3\n",
    "\n",
    "# Define models with default hyperparameters\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    #\"SVM\": SVC(random_state=42),\n",
    "    #\"KNN\": KNeighborsClassifier(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, classification_report(y_train, y_pred_train), classification_report(y_test, y_pred_test)\n",
    "\n",
    "# Loop through each model and evaluate it\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    y_pred_train, y_pred_test, train_report, test_report = evaluate_model(model, X_train_resampled_3, y_train_resampled_3, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"Train Classification Report:\")\n",
    "    print(train_report)\n",
    "    \n",
    "    print(\"Test Classification Report:\")\n",
    "    print(test_report)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (539472, 17)\n",
      "Resampled dataset size: (409890, 17)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Random Undersampling\n",
    "rus = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = rus.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "print(f\"Original dataset size: {X_train_encoded.shape}\")\n",
    "print(f\"Resampled dataset size: {X_train_undersampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dep_Delay_Tag\n",
       "0    204945\n",
       "1    204945\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_undersampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based models with non-standarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467278    0\n",
       "270224    0\n",
       "379329    0\n",
       "186910    0\n",
       "275157    0\n",
       "         ..\n",
       "539463    1\n",
       "539465    1\n",
       "539466    1\n",
       "539468    1\n",
       "539469    1\n",
       "Name: Dep_Delay_Tag, Length: 409890, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RandomForestClassifier...\n",
      "Classification Report (Train) for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    204945\n",
      "           1       1.00      1.00      1.00    204945\n",
      "\n",
      "    accuracy                           1.00    409890\n",
      "   macro avg       1.00      1.00      1.00    409890\n",
      "weighted avg       1.00      1.00      1.00    409890\n",
      "\n",
      "Classification Report (Test) for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71    838275\n",
      "           1       0.54      0.61      0.57    510406\n",
      "\n",
      "    accuracy                           0.65   1348681\n",
      "   macro avg       0.64      0.65      0.64   1348681\n",
      "weighted avg       0.66      0.65      0.66   1348681\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating XGBoostClassifier...\n",
      "Classification Report (Train) for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69    204945\n",
      "           1       0.69      0.65      0.67    204945\n",
      "\n",
      "    accuracy                           0.68    409890\n",
      "   macro avg       0.68      0.68      0.68    409890\n",
      "weighted avg       0.68      0.68      0.68    409890\n",
      "\n",
      "Classification Report (Test) for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72    838275\n",
      "           1       0.55      0.63      0.59    510406\n",
      "\n",
      "    accuracy                           0.67   1348681\n",
      "   macro avg       0.65      0.66      0.65   1348681\n",
      "weighted avg       0.68      0.67      0.67   1348681\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating GradientBoostingClassifier...\n",
      "Classification Report (Train) for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66    204945\n",
      "           1       0.66      0.61      0.63    204945\n",
      "\n",
      "    accuracy                           0.65    409890\n",
      "   macro avg       0.65      0.65      0.65    409890\n",
      "weighted avg       0.65      0.65      0.65    409890\n",
      "\n",
      "Classification Report (Test) for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71    838275\n",
      "           1       0.54      0.61      0.57    510406\n",
      "\n",
      "    accuracy                           0.65   1348681\n",
      "   macro avg       0.64      0.65      0.64   1348681\n",
      "weighted avg       0.66      0.65      0.66   1348681\n",
      "\n",
      "------------------------------------------------------------\n",
      "Evaluating LightGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 204945, number of negative: 204945\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2200\n",
      "[LightGBM] [Info] Number of data points in the train set: 409890, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Classification Report (Train) for LGBMClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.70      0.67    204945\n",
      "           1       0.67      0.62      0.65    204945\n",
      "\n",
      "    accuracy                           0.66    409890\n",
      "   macro avg       0.66      0.66      0.66    409890\n",
      "weighted avg       0.66      0.66      0.66    409890\n",
      "\n",
      "Classification Report (Test) for LGBMClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.70      0.72    838275\n",
      "           1       0.55      0.61      0.58    510406\n",
      "\n",
      "    accuracy                           0.66   1348681\n",
      "   macro avg       0.65      0.65      0.65   1348681\n",
      "weighted avg       0.67      0.66      0.67   1348681\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Random Undersampling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Define models with default hyperparameters\n",
    "models = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoostClassifier\": XGBClassifier(random_state=42),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=42),\n",
    "    \"LightGBMClassifier\": lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Display the classification report for training and test data\n",
    "    print(f\"Classification Report (Train) for {model.__class__.__name__}:\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    \n",
    "    print(f\"Classification Report (Test) for {model.__class__.__name__}:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    evaluate_model(model, X_train_undersampled, y_train_undersampled, X_test_encoded, y_test)\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-Learning (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados do NearMiss no código abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|>>>>>>>>>>| 30/30 [00:17<00:00,  1.70epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69    102436\n",
      "           1       0.69      0.64      0.67    102436\n",
      "\n",
      "    accuracy                           0.68    204872\n",
      "   macro avg       0.68      0.68      0.68    204872\n",
      "weighted avg       0.68      0.68      0.68    204872\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.57      0.63    838275\n",
      "           1       0.47      0.63      0.54    510406\n",
      "\n",
      "    accuracy                           0.59   1348681\n",
      "   macro avg       0.59      0.60      0.59   1348681\n",
      "weighted avg       0.62      0.59      0.60   1348681\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Dense(100, activation='relu', input_shape=(X_train_resampled_1.shape[1],)),  # First hidden layer\n",
    "    Dense(50, activation='relu'),  # Second hidden layer\n",
    "    Dense(25, activation='relu'),  # Third hidden layer\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Total number of desired epochs\n",
    "epochs = 30\n",
    "batch_size = 1024  # Same as in MLPClassifier\n",
    "\n",
    "# Progress bar\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\", ascii=\"->\")\n",
    "\n",
    "# Train manually for multiple epochs\n",
    "for epoch in progress_bar:\n",
    "    model.fit(X_train_resampled_1, y_train_resampled_1, epochs=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_mlp = (model.predict(X_train_resampled_1) > 0.5).astype(int)\n",
    "y_pred_test_mlp = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train Classification Report:\")\n",
    "print(classification_report(y_train_resampled_1, y_pred_train_mlp))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_mlp))\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|>>>>>>>>>>| 30/30 [00:15<00:00,  1.90epoch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68    102436\n",
      "           1       0.69      0.55      0.61    102436\n",
      "\n",
      "    accuracy                           0.65    204872\n",
      "   macro avg       0.65      0.65      0.64    204872\n",
      "weighted avg       0.65      0.65      0.64    204872\n",
      "\n",
      "------------------------------------------------------------\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73    838275\n",
      "           1       0.54      0.51      0.52    510406\n",
      "\n",
      "    accuracy                           0.65   1348681\n",
      "   macro avg       0.63      0.62      0.63   1348681\n",
      "weighted avg       0.65      0.65      0.65   1348681\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# NearMiss3\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\", ascii=\"->\")\n",
    "\n",
    "# Train manually for multiple epochs\n",
    "for epoch in progress_bar:\n",
    "    model.fit(X_train_resampled_3, y_train_resampled_3, epochs=1, batch_size=batch_size, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_mlp = (model.predict(X_train_resampled_3) > 0.5).astype(int)\n",
    "y_pred_test_mlp = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train Classification Report:\")\n",
    "print(classification_report(y_train_resampled_1, y_pred_train_mlp))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_mlp))\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = train_set_subsampled[features_num + features_cat + features_id].copy()\n",
    "X_test_cat = test_set[features_num + features_cat + features_id].copy()\n",
    "y_train_cat = train_set_subsampled[target_class].copy()\n",
    "y_test_cat = test_set[target_class].copy()\n",
    "\n",
    "X_train_cat.drop(columns=[\"DOT_ID_Marketing_Airline\", \"Flight_Number_Marketing_Airline\",  \n",
    "                              \"OriginAirportSeqID\", \"OriginCityMarketID\"], inplace=True)\n",
    "X_test_cat.drop(columns=[\"DOT_ID_Marketing_Airline\", \"Flight_Number_Marketing_Airline\",  \n",
    "                             \"OriginAirportSeqID\", \"OriginCityMarketID\"], inplace=True)\n",
    "\n",
    "categorical_features = [col for col in X_train_cat.columns if col in X_train_cat.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (316625, 24)\n",
      "Resampled dataset size: (138540, 24)\n"
     ]
    }
   ],
   "source": [
    "# Random Undersampling for CatBoost\n",
    "rus_cat = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "X_train_undersampled_cat, y_train_undersampled_cat = rus_cat.fit_resample(X_train_cat, y_train_cat)\n",
    "\n",
    "print(f\"Original dataset size: {X_train_cat.shape}\")\n",
    "print(f\"Resampled dataset size: {X_train_undersampled_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_undersampled_cat[categorical_features] = X_train_undersampled_cat[categorical_features].apply(\n",
    "    lambda col: col.astype(str) if col.dtype != 'int' else col\n",
    ")\n",
    "\n",
    "X_test_cat[categorical_features] = X_test_cat[categorical_features].apply(\n",
    "    lambda col: col.astype(str) if col.dtype != 'int' else col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for CatBoost - Train Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.67      0.66     69270\n",
      "         1.0       0.66      0.64      0.65     69270\n",
      "\n",
      "    accuracy                           0.66    138540\n",
      "   macro avg       0.66      0.66      0.66    138540\n",
      "weighted avg       0.66      0.66      0.66    138540\n",
      "\n",
      "Classification Report for CatBoost - Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.83      0.83    618484\n",
      "         1.0       0.41      0.43      0.42    173081\n",
      "\n",
      "    accuracy                           0.74    791565\n",
      "   macro avg       0.62      0.63      0.63    791565\n",
      "weighted avg       0.75      0.74      0.74    791565\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 1., 0.]), array([0, 0, 1, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Definir o modelo CatBoostClassifier com parâmetros padrão\n",
    "catboost_model = CatBoostClassifier(random_state=42, iterations=100, verbose=0)\n",
    "\n",
    "# Função para avaliar o modelo com threshold de 80% apenas nos dados de teste\n",
    "def evaluate_catboost_model(model, X_train, y_train, X_test, y_test, cat_features=None):\n",
    "    model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)  # Predição normal para dados de treino\n",
    "    y_pred_test_proba = model.predict_proba(X_test)[:, 1]  # Probabilidade da classe 1 para dados de teste\n",
    "\n",
    "    # Aplicar threshold de 80% apenas nos dados de teste: Classe 1 apenas se a probabilidade for > 80%\n",
    "    threshold = 0.6\n",
    "    y_pred_test = (y_pred_test_proba > threshold).astype(int)\n",
    "\n",
    "    # Imprimir o classification report para os dados de treino e teste\n",
    "    print(f\"Classification Report for CatBoost - Train Data:\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    \n",
    "    print(f\"Classification Report for CatBoost - Test Data:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    return y_pred_train, y_pred_test\n",
    "\n",
    "# Avaliar o modelo separadamente\n",
    "evaluate_catboost_model(catboost_model, X_train_undersampled_cat, y_train_undersampled_cat, X_test_cat, y_test_cat, cat_features=categorical_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2ad3fb0e940>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro nas previsões de alta confiança para classe 1: 39.76%\n",
      "Erro nas previsões de alta confiança para classe 0: 6.42%\n"
     ]
    }
   ],
   "source": [
    "# Supondo que 'model' seja o seu RandomForestClassifier treinado\n",
    "# E que X_test seja o conjunto de teste\n",
    "y_proba = catboost_model.predict_proba(X_test_cat)  # Obtém as probabilidades\n",
    "y_pred = np.argmax(y_proba, axis=1)    # Converte para predições binárias (0 ou 1)\n",
    "\n",
    "# Definição de limiar de alta confiança (80%)\n",
    "threshold = 0.8\n",
    "\n",
    "# Filtrando previsões com alta confiança para a classe 1\n",
    "high_conf_1 = y_proba[:, 1] >= threshold\n",
    "y_pred_1 = y_pred[high_conf_1]\n",
    "y_true_1 = y_test[high_conf_1]\n",
    "erro_1 = np.mean(y_pred_1 != y_true_1) * 100  # Percentual de erro para classe 1\n",
    "\n",
    "# Filtrando previsões com alta confiança para a classe 0\n",
    "high_conf_0 = y_proba[:, 0] >= threshold\n",
    "y_pred_0 = y_pred[high_conf_0]\n",
    "y_true_0 = y_test[high_conf_0]\n",
    "erro_0 = np.mean(y_pred_0 != y_true_0) * 100  # Percentual de erro para classe 0\n",
    "\n",
    "print(f\"Erro nas previsões de alta confiança para classe 1: {erro_1:.2f}%\")\n",
    "print(f\"Erro nas previsões de alta confiança para classe 0: {erro_0:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning and Hiperparams Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best parameters found:  {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 15, 'bootstrap': False}\n",
      "Best score found:  0.6458304414214325\n",
      "Classification Report (Train) for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.82      0.78    172896\n",
      "         1.0       0.80      0.73      0.76    172896\n",
      "\n",
      "    accuracy                           0.77    345792\n",
      "   macro avg       0.78      0.77      0.77    345792\n",
      "weighted avg       0.78      0.77      0.77    345792\n",
      "\n",
      "Classification Report (Test) for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.70      0.77    618484\n",
      "         1.0       0.36      0.60      0.45    173081\n",
      "\n",
      "    accuracy                           0.68    791565\n",
      "   macro avg       0.61      0.65      0.61    791565\n",
      "weighted avg       0.75      0.68      0.70    791565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid for RandomForestClassifier\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 150, 200, 300],  # Número de árvores na floresta\n",
    "    \"max_depth\": [5, 10, 15, None],  # Profundidade máxima das árvores\n",
    "    \"min_samples_split\": [2, 5, 10],  # Número mínimo de amostras necessárias para dividir um nó\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Número mínimo de amostras necessárias para estar em um nó folha\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  # Número de recursos a considerar para dividir um nó\n",
    "    \"bootstrap\": [True, False]  # Se as amostras de treino devem ser geradas com reposição\n",
    "}\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=100,  # Número de tentativas aleatórias\n",
    "    cv=3,  # Validação cruzada com 3 divisões\n",
    "    verbose=2,  # Nível de detalhe nos prints durante a busca\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos os núcleos da CPU\n",
    ")\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "random_search.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Print the best parameters and best score found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator from the RandomizedSearchCV\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Now evaluate the best model on the test set\n",
    "evaluate_model(best_rf_model, X_train_undersampled, y_train_undersampled, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters found:  {'subsample': 0.9, 'scale_pos_weight': 10, 'n_estimators': 150, 'min_child_weight': 5, 'max_depth': 10, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 0.7}\n",
      "Best score found:  0.6797433092763089\n",
      "Classification Report (Train) for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.22      0.37     69270\n",
      "         1.0       0.56      1.00      0.72     69270\n",
      "\n",
      "    accuracy                           0.61    138540\n",
      "   macro avg       0.78      0.61      0.54    138540\n",
      "weighted avg       0.78      0.61      0.54    138540\n",
      "\n",
      "Classification Report (Test) for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.13      0.23    618484\n",
      "         1.0       0.24      0.96      0.38    173081\n",
      "\n",
      "    accuracy                           0.31    791565\n",
      "   macro avg       0.58      0.54      0.30    791565\n",
      "weighted avg       0.77      0.31      0.26    791565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 150, 200, 300],\n",
    "    \"max_depth\": [5, 7, 10, 12, 15],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "    \"scale_pos_weight\": [10, 20, 30, 40],\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"min_child_weight\": [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Create the XGBClassifier\n",
    "xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    objective='binary:logitraw',\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "# Create the RandomizedSearchCV object with precision as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    ")\n",
    "\n",
    "# Run RandomizedSearchCV with progress bar\n",
    "random_search.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Print the best parameters and best precision score found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best score found: \", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator from the RandomizedSearchCV\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on both training and test sets\n",
    "evaluate_model(best_xgb_model, X_train_undersampled, y_train_undersampled, X_test_encoded, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters found:  {'subsample': 0.9, 'n_estimators': 150, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10, 'learning_rate': 0.1}\n",
      "Best F1-score found:  0.6666468421156634\n",
      "Classification Report (Train) for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.77      0.75    172896\n",
      "         1.0       0.76      0.72      0.74    172896\n",
      "\n",
      "    accuracy                           0.75    345792\n",
      "   macro avg       0.75      0.75      0.75    345792\n",
      "weighted avg       0.75      0.75      0.75    345792\n",
      "\n",
      "Classification Report (Test) for GradientBoostingClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.69      0.77    618484\n",
      "         1.0       0.36      0.64      0.46    173081\n",
      "\n",
      "    accuracy                           0.68    791565\n",
      "   macro avg       0.62      0.66      0.62    791565\n",
      "weighted avg       0.76      0.68      0.70    791565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for GradientBoostingClassifier\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 150, 200],  # Number of boosting stages (similar to n_estimators in XGBoost)\n",
    "    \"max_depth\": [3, 5, 10],  # Maximum depth of each tree\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],  # Step size shrinking to make the model more robust\n",
    "    \"subsample\": [0.7, 0.8, 0.9],  # Fraction of samples to use for fitting each tree (analogous to subsample in XGBoost)\n",
    "    \"min_samples_split\": [2, 5, 10],  # The minimum number of samples required to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # The minimum number of samples required to be at a leaf node\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],  # The number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    "    loss='log_loss',  # Logarithmic loss (log loss) for binary classification\n",
    ")\n",
    "\n",
    "# Create the RandomizedSearchCV object with F1-score as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    gb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of random attempts\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,  # Print details during the search\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    scoring='precision',  # F1-score for class 1 \n",
    ")\n",
    "\n",
    "\n",
    "# Run RandomizedSearchCV with progress bar\n",
    "random_search.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Print the best parameters and best F1-score found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best F1-score found: \", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator from the RandomizedSearchCV\n",
    "best_gb_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on both training and test sets\n",
    "evaluate_model(best_gb_model, X_train_undersampled, y_train_undersampled, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 69270, number of negative: 69270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3443\n",
      "[LightGBM] [Info] Number of data points in the train set: 138540, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best parameters found:  {'subsample': 0.9, 'scale_pos_weight': 1, 'reg_lambda': 0, 'reg_alpha': 0.1, 'n_estimators': 200, 'min_child_weight': 5, 'min_child_samples': 10, 'max_depth': 12, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "Best F1-score found:  0.6530801184702341\n",
      "[LightGBM] [Info] Number of positive: 69270, number of negative: 69270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3443\n",
      "[LightGBM] [Info] Number of data points in the train set: 138540, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Classification Report (Train) for LGBMClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.71      0.69     69270\n",
      "         1.0       0.69      0.65      0.67     69270\n",
      "\n",
      "    accuracy                           0.68    138540\n",
      "   macro avg       0.68      0.68      0.68    138540\n",
      "weighted avg       0.68      0.68      0.68    138540\n",
      "\n",
      "Classification Report (Test) for LGBMClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.69      0.77    618484\n",
      "         1.0       0.36      0.62      0.45    173081\n",
      "\n",
      "    accuracy                           0.67    791565\n",
      "   macro avg       0.61      0.66      0.61    791565\n",
      "weighted avg       0.76      0.67      0.70    791565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='lightgbm')\n",
    "\n",
    "# Define the parameter grid for LGBMClassifier\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 150, 200, 250, 300],  # Number of boosting iterations (similar to n_estimators in XGBoost)\n",
    "    \"max_depth\": [3, 5, 7, 10, 12, 15],  # Maximum depth of each tree\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],  # Step size shrinking to make the model more robust\n",
    "    \"subsample\": [0.7, 0.8, 0.9],  # Fraction of samples to use for fitting each tree\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],  # Fraction of features to consider when building each tree\n",
    "    \"scale_pos_weight\": [1, 10, 20],  # Used to scale the weight of the positive class (helps with imbalanced classes)\n",
    "    \"min_child_samples\": [10, 20, 30],  # Minimum number of samples a leaf node must have\n",
    "    \"min_child_weight\": [1, 5, 10],  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    \"reg_alpha\": [0, 0.1, 0.5],  # L1 regularization term on weights\n",
    "    \"reg_lambda\": [0, 0.1, 0.5],  # L2 regularization term on weights\n",
    "}\n",
    "\n",
    "# Create the LGBMClassifier\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    objective='binary',  # Binary classification problem\n",
    "    metric='binary_logloss',  # Binary log loss as the evaluation metric\n",
    ")\n",
    "\n",
    "# Create the RandomizedSearchCV object with F1-score as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Number of random attempts\n",
    "    cv=5,  # 3-fold cross-validation\n",
    "    verbose=0,  # Print details during the search\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    scoring='f1_macro',  # F1-score for class 1 \n",
    ")\n",
    "\n",
    "# Run RandomizedSearchCV with progress bar\n",
    "random_search.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Print the best parameters and best F1-score found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best F1-score found: \", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator from the RandomizedSearchCV\n",
    "best_lgbm_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on both training and test sets\n",
    "evaluate_model(best_lgbm_model, X_train_undersampled, y_train_undersampled, X_test_encoded, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "categorical_features = features_cat + features_id\n",
    "\n",
    "# Define the CatBoost model\n",
    "catboost_model = CatBoostClassifier(iterations=1000, depth=10, learning_rate=0.1, loss_function='Logloss', verbose=0)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'iterations': randint(100, 500),  # Number of boosting iterations (lower range)\n",
    "    'depth': randint(3, 8),  # Maximum depth of the trees (smaller range)\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate (fewer options)\n",
    "    'l2_leaf_reg': [1, 5, 10],  # L2 regularization term (fewer options)\n",
    "    'border_count': randint(32, 100),  # Number of splits for numeric features (smaller range)\n",
    "    'max_ctr_complexity': [1, 2]  # Maximum complexity of categorical feature combinations (fewer options)\n",
    "}\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(catboost_model, param_distributions=param_dist, n_iter=20, scoring='f1', cv=3, \n",
    "                                   verbose=0, random_state=42, error_score='raise')\n",
    "\n",
    "# Fit the RandomizedSearchCV to find the best parameters\n",
    "print(\"Starting Randomized Search...\")\n",
    "random_search.fit(X_train, y_train, cat_features=categorical_features)\n",
    "\n",
    "# Output the best parameters and the best score\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best Score: {random_search.best_score_}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test Set Accuracy: {test_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  # Suppresses warnings\n",
    "\n",
    "\n",
    "# Define the model with the updated parameters\n",
    "mlp = MLPClassifier(\n",
    "    random_state=42,                    # Ensures reproducibility\n",
    "    warm_start=False                    # No incremental training, starting fresh each time\n",
    ")\n",
    "\n",
    "# Define the hyperparameters for RandomizedSearchCV\n",
    "# param_dist = {\n",
    "#     'hidden_layer_sizes': [(100,), (100, 50), (100, 50, 25), (200, 100, 50)],\n",
    "#     'activation': ['relu', 'tanh'],\n",
    "#     'solver': ['adam', 'sgd'],\n",
    "#     'batch_size': [128, 512, 1024],\n",
    "#     'max_iter': [10, 50, 100],\n",
    "# }\n",
    "\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50), (100, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'batch_size': [128, 1024],\n",
    "    'max_iter': [10, 50],\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=3, random_state=42, verbose=1)\n",
    "\n",
    "# Use tqdm to create a friendly progress bar for the training loop\n",
    "epochs = 50\n",
    "progress_bar = tqdm(range(epochs), desc=\"Training Progress\", unit=\"epoch\", ascii=\"->\")\n",
    "\n",
    "# NearMiss 1\n",
    "for _ in progress_bar:\n",
    "    random_search.fit(X_train_resampled_1, y_train_resampled_1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_mlp = random_search.predict(X_train_resampled_1)\n",
    "y_pred_test_mlp = random_search.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Train Classification Report:\")\n",
    "print(classification_report(y_train_resampled_1, y_pred_train_mlp))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test_mlp))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU-TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
